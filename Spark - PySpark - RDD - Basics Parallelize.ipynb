{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyp692d/mgP8v9vj9GcVUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/pyspark/blob/main/Spark%20-%20PySpark%20-%20RDD%20-%20Basics%20Parallelize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark - PySpark - RDD - Basics Parallelize**"
      ],
      "metadata": {
        "id": "ok4SNAETG1ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD** is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster."
      ],
      "metadata": {
        "id": "5KjdxKVuHCzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install PySpark**"
      ],
      "metadata": {
        "id": "T_eQP2cPHZ1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasVbBH_Gs04",
        "outputId": "68d4bfe9-06f2-41be-89ee-970224a374fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 49 kB/s \n",
            "\u001b[?25hCollecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 92.0 MB/s \n",
            "\u001b[?25h  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 15.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=f522286b28f14950a4e4d5db920b787c568d7e9925d2a0ec177612bcce647256\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Function**"
      ],
      "metadata": {
        "id": "0eSlSR-JHgO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "w_vApqCnHwFI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "        .appName(\"RDDParallelize\") \\\n",
        "        .getOrCreate()        "
      ],
      "metadata": {
        "id": "JzcVX-TRIPPn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `sparkContext.parallelize()` to create RDD, this function also has another signature which additionally takes integer argument to specifies the number of partitions. Partitions are basic units of parallelism in Apache Spark. RDDs in Apache Spark are a collection of partitions."
      ],
      "metadata": {
        "id": "4s5J6_kPLx18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4 ,5], 2)"
      ],
      "metadata": {
        "id": "Q3GkGYv9I1Hk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collect = rdd.collect()"
      ],
      "metadata": {
        "id": "oEhDhFrGJHF4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Collection\n",
        "print(\"# of partitions: {}\".format(rdd.getNumPartitions()))\n",
        "print(\"Action: 1st Element: {}\".format(rdd.first()))\n",
        "print(\"Action: Converted To Array: {}\".format(collect))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAwr0QFjJMoV",
        "outputId": "a9b38d0b-7b6e-4721-8696-63996be7bd9f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of partitions: 2\n",
            "Action: 1st Element: 1\n",
            "Action: Converted To Array: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    }
  ]
}